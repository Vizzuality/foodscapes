{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual layers\n",
    "\n",
    "Get contextual layers from source and save them as `MBTiles` or `COGs`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create_mbtiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mbtiles(source_path, dest_path, layer_name, opts=\"-zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel\"):\n",
    "    \"\"\"\n",
    "    Use tippecanoe to create a MBTILE at dest_path from source_path.\n",
    "    layer_name is used for the name of the layer in the MBTILE.\n",
    "    Regex file path (/*.geojson) is supported for source_path.\n",
    "    \"\"\"\n",
    "    cmd = f\"tippecanoe -o {dest_path} -l {layer_name} {opts} {source_path}\"\n",
    "    print(f\"Processing: {cmd}\")\n",
    "    r = subprocess.call(cmd, shell=True)\n",
    "    if r == 0:\n",
    "        print(\"Task created\")\n",
    "    else:\n",
    "        print(\"Task failed\")\n",
    "    print(\"Finished processing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create_COG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_COG(source_path, dest_path, file_name, description):\n",
    "    \"\"\"\n",
    "    Use gdal_translate to create a COG at dest_path from source_path.\n",
    "    \"\"\"\n",
    "    # Include internal overviews\n",
    "    #cmd_internal_overviews = f\"gdaladdo -r average {source_path} 2 4 8 16\"\n",
    "    #cmd_create_cog = f\"gdal_translate -co TILED=YES -co COPY_SRC_OVERVIEWS=YES -co COMPRESS=LZW {source_path} {dest_path}\"\n",
    "    cmd_create_cog =  f\"gdal_translate -of COG -co TILING_SCHEME=GoogleMapsCompatible -co COMPRESS=LZW -co ZOOM_LEVEL_STRATEGY='lower' {source_path} {dest_path}\"\n",
    "    #cmd_create_cog =  f\"gdalwarp -t_srs EPSG:4326 -co TILED=YES -co COMPRESS=LZW -co COPY_SRC_OVERVIEWS=YES {source_path} {dest_path}\"\n",
    "    cmd_metadata = f\"gdal_edit.py -mo \\\"TIFFTAG_DOCUMENTNAME={file_name}\\\" -mo \\\"TIFFTAG_IMAGEDESCRIPTION={description}\\\" {dest_path}\"\n",
    "    for cmd in [cmd_create_cog]:\n",
    "        print(f\"Processing: {cmd}\")\n",
    "        r = subprocess.call(cmd, shell=True)\n",
    "        if r == 0:\n",
    "            print(\"Task created\")\n",
    "        else:\n",
    "            print(\"Task failed\")\n",
    "\n",
    "    print(\"Finished processing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from different sources\n",
    "### World Database on Protected Areas ([source](https://www.protectedplanet.net/en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WDPA_WDOECM_May2023_Public_all_shp.zip: 99.4MB [00:07, 13.6MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Download the file with progress bar\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit_divisor\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, miniters\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, desc\u001b[39m=\u001b[39murl\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mas\u001b[39;00m t:\n\u001b[0;32m---> 16\u001b[0m     urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlretrieve(url, filename\u001b[39m=\u001b[39;49mfile_name, reporthook\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x, y, z: t\u001b[39m.\u001b[39;49mupdate(y))\n\u001b[1;32m     18\u001b[0m \u001b[39m# Extract the contents of the zip file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(file_name, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m zip_ref:\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/urllib/request.py:277\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m             blocknum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    276\u001b[0m             \u001b[39mif\u001b[39;00m reporthook:\n\u001b[0;32m--> 277\u001b[0m                 reporthook(blocknum, bs, size)\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m read \u001b[39m<\u001b[39m size:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m ContentTooShortError(\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mretrieval incomplete: got only \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m out of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m%\u001b[39m (read, size), result)\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x, y, z)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Download the file with progress bar\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit_divisor\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, miniters\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, desc\u001b[39m=\u001b[39murl\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mas\u001b[39;00m t:\n\u001b[0;32m---> 16\u001b[0m     urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlretrieve(url, filename\u001b[39m=\u001b[39mfile_name, reporthook\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x, y, z: t\u001b[39m.\u001b[39;49mupdate(y))\n\u001b[1;32m     18\u001b[0m \u001b[39m# Extract the contents of the zip file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(file_name, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m zip_ref:\n",
      "File \u001b[0;32m~/anaconda3/envs/geopy11/lib/python3.11/site-packages/tqdm/std.py:1247\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39m# check counter first to reduce calls to time()\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mminiters:\n\u001b[0;32m-> 1247\u001b[0m     cur_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_time()\n\u001b[1;32m   1248\u001b[0m     dt \u001b[39m=\u001b[39m cur_t \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_t\n\u001b[1;32m   1249\u001b[0m     \u001b[39mif\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmininterval \u001b[39mand\u001b[39;00m cur_t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_t \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelay:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_folder = 'data/contextual_layers/'\n",
    "if not os.path.exists(os.path.join(data_folder, 'raw')):\n",
    "    os.makedirs(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "now = datetime.datetime.now()\n",
    "month_name = now.strftime(\"%b\")\n",
    "year = now.year\n",
    "\n",
    "date = f\"{month_name}{year}\"\n",
    "\n",
    "file_name = os.path.join(data_folder, 'raw', f\"WDPA_WDOECM_{date}_Public_all_shp.zip\")\n",
    "url = f\"https://d1gam3xoknrgr2.cloudfront.net/current/WDPA_WDOECM_{date}_Public_all_shp.zip\"\n",
    "\n",
    "# Download the file with progress bar\n",
    "with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "    urllib.request.urlretrieve(url, filename=file_name, reporthook=lambda x, y, z: t.update(y))\n",
    "\n",
    "# Extract the contents of the zip file\n",
    "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "# Read and process shapefile\n",
    "file_name = os.path.join(data_folder, 'raw', f'WDPA_WDOECM_{date}_Public_all_shp_1.zip')\n",
    "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_folder, 'raw'))\n",
    "    shp_file = [f for f in zip_ref.namelist() if f.endswith('.shp')][0]\n",
    "    with tqdm(total=1, desc=f'Reading and simplifying shapefile') as pbar:\n",
    "        df = gpd.read_file(os.path.join(data_folder, 'raw', shp_file), callback=lambda x: pbar.update(1))\n",
    "        df = df.to_crs('epsg:4326')\n",
    "        df['geometry'] = df['geometry'].simplify(tolerance=0.001, \n",
    "                                                preserve_topology=True)\n",
    "        df = df[['WDPAID', 'NAME', 'IUCN_CAT', 'STATUS', 'DESIG_TYPE',\n",
    "                    'GOV_TYPE', 'OWN_TYPE', 'MANG_AUTH', 'geometry']]\n",
    "\n",
    "# Use shutil.rmtree to remove the data_folder and all its contents\n",
    "shutil.rmtree(os.path.join(data_folder, 'raw'))\n",
    "\n",
    "# Save the final GeoDataFrame as a GeoJSON file\n",
    "output_file = os.path.join(data_folder, 'protected_areas.json')\n",
    "df.to_file(output_file, driver='GeoJSON')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### River basins \n",
    "- Major hydrological basins of the world ([source](https://data.apps.fao.org/catalog//iso/7707086d-af3c-41cc-8aa5-323d8609b2d1))\n",
    "- Hydrological basins of the world ([source](https://data.apps.fao.org/catalog/iso/f2615a41-6383-4aa4-aa21-743330eb03ae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hydro_basins_world.zip: 54.6MB [00:06, 8.21MB/s]\n",
      "Reading shapefile:   0%|          | 0/1 [00:08<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/contextual_layers/'\n",
    "if not os.path.exists(os.path.join(data_folder, 'raw')):\n",
    "    os.makedirs(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "file_name = os.path.join(data_folder, 'raw', \"hydro_basins_world.zip\")\n",
    "url = \"https://storage.googleapis.com/810c63d8-3fde-4ecd-9882-14d62e3058be/static/downloads/HydroBasins/hydro_basins_world.zip\"\n",
    "\n",
    "# Download the file with progress bar\n",
    "with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "    urllib.request.urlretrieve(url, filename=file_name, reporthook=lambda x, y, z: t.update(y))\n",
    "\n",
    "# Extract the contents of the zip file\n",
    "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "# Read shapefile\n",
    "shp_file = \"SUB_BAS.shp\"\n",
    "with tqdm(total=1, desc=f'Reading shapefile') as pbar:\n",
    "    df = gpd.read_file(os.path.join(data_folder, 'raw', shp_file), callback=lambda x: pbar.update(1))\n",
    "    \n",
    "# Use shutil.rmtree to remove the data_folder and all its contents\n",
    "shutil.rmtree(os.path.join(data_folder, 'raw'))\n",
    "\n",
    "# Save the final GeoDataFrame as a GeoJSON file\n",
    "output_file = os.path.join(data_folder, 'hydrological_basins.json')\n",
    "df.to_file(output_file, driver='GeoJSON')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrecoverable carbon in Earth's ecosystems ([source](https://zenodo.org/record/4091029#.ZEI9ctKZOUl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Irrecoverable_Carbon_2018.zip?download=1: 1.88GB [17:10, 1.96MB/s]\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/contextual_layers/'\n",
    "if not os.path.exists(os.path.join(data_folder, 'raw')):\n",
    "    os.makedirs(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "file_name = os.path.join(data_folder, 'raw', \"Irrecoverable_Carbon_2018.zip\")\n",
    "url = \"https://zenodo.org/record/4091029/files/Irrecoverable_Carbon_2018.zip?download=1\"\n",
    "\n",
    "# Download the file with progress bar\n",
    "with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "    urllib.request.urlretrieve(url, filename=file_name, reporthook=lambda x, y, z: t.update(y))\n",
    "    \n",
    "# Extract the contents of the zip file\n",
    "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "# Move the file to the destination directory\n",
    "tif_file = \"Irrecoverable_Carbon_2018/Irrecoverable_C_Total_2018.tif\"\n",
    "shutil.move(os.path.join(data_folder, 'raw', tif_file), data_folder)\n",
    "\n",
    "# Use shutil.rmtree to remove the data_folder and all its contents\n",
    "shutil.rmtree(os.path.join(data_folder, 'raw'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Gridded Relative Deprivation Index ([source](https://sedac.ciesin.columbia.edu/data/set/povmap-grdi-v1/data-download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "povmap-grdi-v1-grdiv1-geotiff.zip: 24.0kB [00:01, 23.3kB/s]\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBadZipFile\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m     urllib\u001B[39m.\u001B[39mrequest\u001B[39m.\u001B[39murlretrieve(url, filename\u001B[39m=\u001B[39mfile_name, reporthook\u001B[39m=\u001B[39m\u001B[39mlambda\u001B[39;00m x, y, z: t\u001B[39m.\u001B[39mupdate(y))\n\u001B[1;32m     12\u001B[0m \u001B[39m# Extract the contents of the zip file\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[39mwith\u001B[39;00m zipfile\u001B[39m.\u001B[39;49mZipFile(file_name, \u001B[39m'\u001B[39;49m\u001B[39mr\u001B[39;49m\u001B[39m'\u001B[39;49m) \u001B[39mas\u001B[39;00m zip_ref:\n\u001B[1;32m     14\u001B[0m     zip_ref\u001B[39m.\u001B[39mextractall(os\u001B[39m.\u001B[39mpath\u001B[39m.\u001B[39mjoin(data_folder, \u001B[39m'\u001B[39m\u001B[39mraw\u001B[39m\u001B[39m'\u001B[39m))\n\u001B[1;32m     16\u001B[0m \u001B[39m# Move the file to the destination directory\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/geopy11/lib/python3.11/zipfile.py:1299\u001B[0m, in \u001B[0;36mZipFile.__init__\u001B[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001B[0m\n\u001B[1;32m   1297\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   1298\u001B[0m     \u001B[39mif\u001B[39;00m mode \u001B[39m==\u001B[39m \u001B[39m'\u001B[39m\u001B[39mr\u001B[39m\u001B[39m'\u001B[39m:\n\u001B[0;32m-> 1299\u001B[0m         \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_RealGetContents()\n\u001B[1;32m   1300\u001B[0m     \u001B[39melif\u001B[39;00m mode \u001B[39min\u001B[39;00m (\u001B[39m'\u001B[39m\u001B[39mw\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mx\u001B[39m\u001B[39m'\u001B[39m):\n\u001B[1;32m   1301\u001B[0m         \u001B[39m# set the modified flag so central directory gets written\u001B[39;00m\n\u001B[1;32m   1302\u001B[0m         \u001B[39m# even if no files are added to the archive\u001B[39;00m\n\u001B[1;32m   1303\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_didModify \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/geopy11/lib/python3.11/zipfile.py:1366\u001B[0m, in \u001B[0;36mZipFile._RealGetContents\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1364\u001B[0m     \u001B[39mraise\u001B[39;00m BadZipFile(\u001B[39m\"\u001B[39m\u001B[39mFile is not a zip file\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m   1365\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m endrec:\n\u001B[0;32m-> 1366\u001B[0m     \u001B[39mraise\u001B[39;00m BadZipFile(\u001B[39m\"\u001B[39m\u001B[39mFile is not a zip file\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m   1367\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdebug \u001B[39m>\u001B[39m \u001B[39m1\u001B[39m:\n\u001B[1;32m   1368\u001B[0m     \u001B[39mprint\u001B[39m(endrec)\n",
      "\u001B[0;31mBadZipFile\u001B[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/contextual_layers/'\n",
    "if not os.path.exists(os.path.join(data_folder, 'raw')):\n",
    "    os.makedirs(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "file_name = os.path.join(data_folder, 'raw', \"povmap-grdi-v1-grdiv1-geotiff.zip\")\n",
    "url = \"https://sedac.ciesin.columbia.edu/downloads/data/povmap/povmap-grdi-v1/povmap-grdi-v1-grdiv1-geotiff.zip\"\n",
    "\n",
    "# Download the file with progress bar\n",
    "with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "    urllib.request.urlretrieve(url, filename=file_name, reporthook=lambda x, y, z: t.update(y))\n",
    "    \n",
    "# Extract the contents of the zip file\n",
    "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_folder, 'raw'))\n",
    "    \n",
    "# Move the file to the destination directory\n",
    "tif_file = \"povmap-grdi-v1-grdiv1-geotiff/povmap-grdi-v1.tif\"\n",
    "shutil.move(os.path.join(data_folder, 'raw', tif_file), data_folder)\n",
    "\n",
    "# Use shutil.rmtree to remove the data_folder and all its contents\n",
    "shutil.rmtree(os.path.join(data_folder, 'raw'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `MBTiles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protected areas\n",
      "Processing: tippecanoe -o data/contextual_layers/mbtiles/protected_areas.mbtiles -l Protected areas -zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel data/contextual_layers/protected_areas.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "areas: No such file or directory\n",
      "data/contextual_layers/protected_areas.json:16975: Reached EOF without all containers being closed\n",
      "In JSON object {\"type\":\"FeatureCollection\",\"name\":\"protected_areas\",\"features\":[]}\n",
      "data/contextual_layers/protected_areas.json:3366: Found ] at top level\n",
      "217424 features, 100418953 bytes of geometry, 11678921 bytes of separate metadata, 15612546 bytes of string pool\n",
      "Choosing a maxzoom of -z4 for features about 18791 feet (5728 meters) apart\n",
      "Choosing a maxzoom of -z8 for resolution of about 1567 feet (477 meters) within features\n",
      "tile 0/0/0 size is 1077078 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 41.78% of the features to make it fit\n",
      "tile 0/0/0 size is 963302 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 19.52% of the features to make it fit\n",
      "tile 0/0/0 size is 834220 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 10.53% of the features to make it fit\n",
      "tile 0/0/0 size is 671448 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 7.06% of the features to make it fit\n",
      "tile 0/0/0 size is 554710 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 5.72% of the features to make it fit\n",
      "tile 1/0/0 size is 675594 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 66.61% of the features to make it fit\n",
      "tile 1/1/0 size is 1211818 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 37.13% of the features to make it fit\n",
      "tile 1/0/0 size is 619094 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 48.42% of the features to make it fit\n",
      "tile 1/0/0 size is 590808 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 36.88% of the features to make it fit\n",
      "tile 1/1/0 size is 966373 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 17.29% of the features to make it fit\n",
      "tile 1/0/0 size is 556575 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 29.82% of the features to make it fit\n",
      "tile 1/1/0 size is 726480 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 10.71% of the features to make it fit\n",
      "tile 1/0/0 size is 525198 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 25.55% of the features to make it fit\n",
      "tile 1/1/0 size is 566638 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 8.51% of the features to make it fit\n",
      "tile 2/1/1 size is 758143 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 59.36% of the features to make it fit\n",
      "tile 2/1/1 size is 657913 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 40.60% of the features to make it fit\n",
      "tile 2/2/1 size is 1822732 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 24.69% of the features to make it fit\n",
      "tile 2/1/1 size is 587727 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 31.08% of the features to make it fit\n",
      "tile 2/1/1 size is 532379 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 26.27% of the features to make it fit\n",
      "tile 2/2/1 size is 1053912 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 10.54% of the features to make it fit\n",
      "tile 2/2/1 size is 615281 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 7.71% of the features to make it fit\n",
      "tile 3/4/2 size is 2538962 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 17.72% of the features to make it fit\n",
      "tile 3/4/2 size is 973939 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 8.19% of the features to make it fit\n",
      "tile 3/4/2 size is 537200 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 6.86% of the features to make it fit\n",
      "tile 4/4/5 size is 573303 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 78.49% of the features to make it fit\n",
      "tile 4/9/4 size is 582035 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 77.31% of the features to make it fit\n",
      "tile 4/7/5 size is 512542 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 87.80% of the features to make it fit\n",
      "tile 4/9/5 size is 576833 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 78.01% of the features to make it fit\n",
      "tile 4/8/4 size is 668376 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 67.33% of the features to make it fit\n",
      "tile 4/9/4 size is 509975 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 68.22% of the features to make it fit\n",
      "tile 4/4/5 size is 555761 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 63.56% of the features to make it fit\n",
      "tile 4/8/4 size is 526560 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 57.54% of the features to make it fit\n",
      "tile 4/4/5 size is 521280 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 54.86% of the features to make it fit\n",
      "tile 4/8/5 size is 2368039 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 19.00% of the features to make it fit\n",
      "tile 4/8/5 size is 796804 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 10.73% of the features to make it fit\n",
      "tile 5/17/9 size is 509693 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 88.29% of the features to make it fit\n",
      "tile 5/15/10 size is 581801 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 77.35% of the features to make it fit\n",
      "tile 5/17/11 size is 695461 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 64.71% of the features to make it fit\n",
      "tile 5/17/10 size is 826796 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 54.43% of the features to make it fit\n",
      "tile 5/16/11 size is 885106 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 50.84% of the features to make it fit\n",
      "tile 5/17/11 size is 537273 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 54.19% of the features to make it fit\n",
      "tile 5/17/10 size is 541742 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 45.21% of the features to make it fit\n",
      "tile 5/16/11 size is 642247 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 35.62% of the features to make it fit\n",
      "tile 5/16/10 size is 1464456 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 30.73% of the features to make it fit\n",
      "tile 5/16/11 size is 508146 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 31.55% of the features to make it fit\n",
      "tile 5/16/10 size is 603686 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 22.91% of the features to make it fit\n",
      "tile 6/33/22 size is 790557 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 56.92% of the features to make it fit\n",
      "tile 6/34/21 size is 584786 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 76.95% of the features to make it fit\n",
      "tile 6/33/22 size is 569377 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 44.99% of the features to make it fit\n",
      "tile 6/33/21 size is 1450286 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 31.03% of the features to make it fit\n",
      "tile 6/33/21 size is 612271 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 22.80% of the features to make it fit\n",
      "tile 7/66/42 size is 659185 with detail 12, >500000    \n",
      "Going to try keeping the sparsest 68.27% of the features to make it fit\n",
      "  99.9%  8/125/77   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task created\n",
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "layers = {'Hydrological basins': 'hydrological_basins.json',\n",
    "          'Protected areas': 'protected_area_simp.json'}\n",
    "\n",
    "data_folder = 'data/contextual_layers'\n",
    "if not os.path.exists(os.path.join(data_folder, 'mbtiles')):\n",
    "    os.makedirs(os.path.join(data_folder, 'mbtiles'))\n",
    "\n",
    "for layer_name, file in layers.items():\n",
    "    print(layer_name)\n",
    "    source_path = os.path.join(data_folder, file)\n",
    "    dest_path = os.path.join(data_folder, 'mbtiles', file).split('.')[0]+\".mbtiles\"\n",
    "    create_mbtiles(source_path, dest_path, layer_name, opts=\"-zg --drop-densest-as-needed --extend-zooms-if-still-dropping --force --read-parallel\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `COGs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irrecoverable carbon\n",
      "Processing: gdal_translate -of COG -co TILING_SCHEME=GoogleMapsCompatible -co COMPRESS=LZW -co ZOOM_LEVEL_STRATEGY='lower' data/contextual_layers/Irrecoverable_C_Total_2018.tif data/contextual_layers/cogs/irrecoverable_carbon.tif\n",
      "Input file size is 133584, 53434\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Task created\n",
      "Finished processing\n",
      "Relative deprivation index\n",
      "Processing: gdal_translate -of COG -co TILING_SCHEME=GoogleMapsCompatible -co COMPRESS=LZW -co ZOOM_LEVEL_STRATEGY='lower' data/contextual_layers/povmap-grdi-v1.tif data/contextual_layers/cogs/relative_deprivation_index.tif\n",
      "Input file size is 43178, 16580\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Task created\n",
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "layers = {'Irrecoverable carbon': 'Irrecoverable_C_Total_2018.tif',\n",
    "          'Relative deprivation index': 'povmap-grdi-v1.tif'}\n",
    "\n",
    "rename_file = {'Irrecoverable carbon': 'irrecoverable_carbon.tif',\n",
    "               'Relative deprivation index': 'relative_deprivation_index.tif'}\n",
    "\n",
    "data_folder = 'data/contextual_layers'\n",
    "if not os.path.exists(os.path.join(data_folder, 'cogs')):\n",
    "    os.makedirs(os.path.join(data_folder, 'cogs'))\n",
    "\n",
    "for layer_name, input_file in layers.items():\n",
    "    print(layer_name)\n",
    "    output_file = rename_file[layer_name]\n",
    "    source_path = os.path.join(data_folder, input_file)\n",
    "    dest_path = os.path.join(data_folder, 'cogs', output_file)\n",
    "    create_COG(source_path, dest_path, output_file, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopy11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
